<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Human-level 3D shape perception emerges from multi-view learning</title>
    <link rel="stylesheet" href="static/css/style.css?v=33">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+4:opsz,wght@8..60,300;8..60,400;8..60,600;8..60,700&family=Inter:wght@300;400;500;600&family=JetBrains+Mono:wght@400&display=swap" rel="stylesheet">
</head>
<body>

<!-- ============================================================ -->
<!-- HERO / TITLE -->
<!-- ============================================================ -->
<header id="hero" class="alt-bg">
    <div class="container">
        <h1>Human-level 3D shape perception<br>emerges from multi-view learning</h1>
        <p class="authors">
            <a href="#">Tyler Bonnen</a>,
            <a href="#">Jitendra Malik</a>, and
            <a href="#">Angjoo Kanazawa</a>
        </p>
        <p class="affiliation">University of California, Berkeley</p>
        <div class="hero-links">
            <a href="#" class="btn"><svg class="btn-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M4 19.5A2.5 2.5 0 0 1 6.5 17H20"/><path d="M6.5 2H20v20H6.5A2.5 2.5 0 0 1 4 19.5v-15A2.5 2.5 0 0 1 6.5 2z"/><path d="M8 7h8M8 11h8M8 15h5"/></svg>Paper</a>
            <a href="https://github.com/tzler/human_multiview" class="btn"><svg class="btn-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0 0 24 12c0-6.63-5.37-12-12-12z"/></svg>Code</a>
            <a href="#" class="btn"><svg class="btn-icon" viewBox="0 0 24 24" fill="currentColor"><path d="M6.5 2C4.01 2 2 4.01 2 6.5S4.01 11 6.5 11 11 8.99 11 6.5 8.99 2 6.5 2zm0 7C5.12 9 4 7.88 4 6.5S5.12 4 6.5 4 9 5.12 9 6.5 7.88 9 6.5 9zm11-7C15.01 2 13 4.01 13 6.5s2.01 4.5 4.5 4.5S22 8.99 22 6.5 19.99 2 17.5 2zm0 7C16.12 9 15 7.88 15 6.5S16.12 4 17.5 4 20 5.12 20 6.5 19.88 9 17.5 9zM12 13c-2.49 0-4.5 2.01-4.5 4.5S9.51 22 12 22s4.5-2.01 4.5-4.5S14.49 13 12 13zm0 7c-1.38 0-2.5-1.12-2.5-2.5S10.62 15 12 15s2.5 1.12 2.5 2.5S13.38 20 12 20z"/></svg>Data</a>
        </div>
    </div>
</header>

<!-- ============================================================ -->
<!-- INTRO -->
<!-- ============================================================ -->
<section id="intro" class="section">
    <div class="container narrow">
        <p class="lede">
            How do humans perceive the three-dimensional structure of objects
            from two-dimensional visual inputs?
        </p>
        <p>
            Understanding this ability has been a longstanding goal for both the science and engineering of visual intelligence, 
            yet decades of computational methods have fallen short of human performance. 
            Here we show that human-level 3D shape inferences emerge naturally when training neural networks using a 
            visual-spatial learning objective over naturalistic sensory data. 
            These results provide a bridge between cognitive theories and current practice in deep learning, revealing a novel route towards more human-like vision models.
        </p>
    </div>
</section>

<!-- ============================================================ -->
<!-- SECTION 1: INTERACTIVE ODDITY DEMO (the hook) -->
<!-- ============================================================ -->
<section id="oddity-demo" class="section alt-bg">
    <div class="container">
        <h2>Let's begin with a simple test from the cognitive sciences</h2>
        <p class="section-subtitle">
            To understand human perceptual processes, the cognitive sciences have developed incisive tests of human abilities. 
            This is a good one: Can you identify which of these images contains an object that is different from the other two? 
            Each row shows three images. Two depict the same object from different
            viewpoints; one depicts a different object. Click the odd one out.
        </p>

        <div class="condition-tabs" id="condition-tabs">
            <button class="condition-tab active" data-trial="0">Abstract shapes</button>
            <button class="condition-tab" data-trial="1">Familiar objects</button>
            <button class="condition-tab" data-trial="2">Novel objects</button>
            <button class="condition-tab" data-trial="3">Familiar objects (hard)</button>
        </div>

        <div id="demo-trial" class="oddity-grid">
            <!-- Populated dynamically by JS -->
        </div>

        <div id="demo-feedback" class="feedback hidden">
            <p id="demo-result"></p>
            <p id="demo-stats" class="feedback-stats"></p>
        </div>

        <p class="section-note">
            These trials come from the <a href="https://huggingface.co/datasets/tzler/MOCHI">MOCHI benchmark</a>&mdash;a
            large-scale dataset with over 2,000 trials and behavioral data from
            more than 300 human participants.
        </p>
    </div>
</section>

<!-- ============================================================ -->
<!-- SECTION 2: THE GAP (machines can't do this) -->
<!-- ============================================================ -->
<section id="mochi-gap" class="section">
    <div class="container narrow">
        <h2>Standard vision models fail at this task</h2>
        <p>
            These kinds of tasks are no problem for humans, but standard vision models like DINOv2 and CLIP
            fail to get close to human accuracy on these tasks. Even when we scale up model size. 
            We've observed this failure across a wide range of architectures, training objectives, and datasets. 
            The breadth of this human-model gap encourages us to consider what kind of learning gives rise to 3D perception in the first place.
        </p>
        <div class="figure-single">
            <img src="static/media/mochi_model_failure.png" alt="Model performance vs. scale on MOCHI benchmark" class="figure-img figure-img-medium">
            <p class="caption">
                Scaling up model size improves performance on 3D perception tasks from the cognitive sciences,
                but even the largest models fall far short of human accuracy (dashed line).
            </p>
        </div>
    </div>
</section>

<!-- ============================================================ -->
<!-- SECTION 2b: THEORETICAL FRAMING (why do they fail?) -->
<!-- ============================================================ -->
<section id="theories" class="section alt-bg">
    <div class="container">
        <h2>Cognitive theories offer some explanations</h2>
        <p>
            Understanding why existing models fail leads us to competing theories about how humans
            develop this ability.  
        </p>
        <p>
            <strong>Nativists</strong> argue that perceiving 3D structure requires
            built-in, domain-specific knowledge&mdash;innate priors that provide the constraints necessary for learning. Under this view,
            models fail because they lack the right inductive biases.
        </p>
        <p>
            <strong>Empiricists</strong> argue that 3D perception emerges from
            general-purpose learning over natural sensory experience.
            Under this view, standard models fail because they learn from the wrong data&mdash;static
            images stripped of the spatial structure.
        </p>
    </div>
</section>

<!-- ============================================================ -->
<!-- SECTION 3: THE CLUE (human sensory experience) -->
<!-- ============================================================ -->
<section id="learning-data" class="section">
    <div class="container narrow">
        <h2>Do models need more grounded visual data?</h2>
        <p>
            As infants, we don&rsquo;t learn about objects from isolated snapshots.
            We explore&mdash;crawling, reaching, turning our heads&mdash;and
            receive a continuous stream of multi-modal sensory data: retinal
            images, binocular depth, and vestibular self-motion signals, all
            generated by our own movement through the world.
        </p>

        <div class="sensory-signals" id="sensory-signals">
            <div class="sensory-col">
                <div class="sensory-label">Vision <span class="sensory-source">(retinal)</span></div>
                <video id="sensory-video" loop muted playsinline preload="auto" class="sensory-media">
                    <source src="static/media/headcam_square_web.mp4" type="video/mp4">
                </video>
            </div>
            <div class="sensory-col">
                <div class="sensory-label">Depth <span class="sensory-source">(stereo)</span></div>
                <video id="sensory-depth" loop muted playsinline preload="auto" class="sensory-media">
                    <source src="static/media/headcam_depth_web.mp4" type="video/mp4">
                </video>
            </div>
            <div class="sensory-col">
                <div class="sensory-label">Self-motion <span class="sensory-source">(vestibular)</span></div>
                <canvas id="sensory-motion" class="sensory-media"></canvas>
            </div>
        </div>
        <p class="caption" style="text-align:center; max-width:640px; margin:0.8rem auto 0;">
            Head-mounted cameras on infants capture the visual experience
            that drives perceptual development. From this data, multi-view
            models can recover depth and camera motion&mdash;signals
            analogous to stereo vision and vestibular input.
        </p>
    </div>
</section>

<!-- ============================================================ -->
<!-- SECTION 4: THE INTERVENTION (new class of models) -->
<!-- ============================================================ -->
<section id="modeling" class="section alt-bg">
    <div class="container">
        <h2>A new class of vision models learns from similar sensory data</h2>
        <p>
            A recent class of vision transformers learns from precisely this kind of structured visual-spatial data â€” 
            and optimizes for the same signals that are available to biological visual systems.
            Given sets of images from different viewpoints, these models learn to predict related spatial information 
            such as depth, camera pose, or geometric correspondence. Critically, a generic
            transformer architecture learns all of this with no hand-coded
            geometric priors&mdash;any understanding of 3D structure emerges
            from the predictive relationship between images and spatial
            information.
        </p>

        <div class="sensory-signals" id="model-signals">
            <div class="sensory-col">
                <div class="sensory-label">Input images</div>
                <video id="model-video" loop muted playsinline preload="auto" class="sensory-media">
                    <source src="static/media/greenhouse_rgb_web.mp4" type="video/mp4">
                </video>
            </div>
            <div class="sensory-col">
                <div class="sensory-label">Predicted depth</div>
                <video id="model-depth" loop muted playsinline preload="auto" class="sensory-media">
                    <source src="static/media/greenhouse_depth_web.mp4" type="video/mp4">
                </video>
            </div>
            <div class="sensory-col">
                <div class="sensory-label">Predicted cameras</div>
                <canvas id="model-motion" class="sensory-media"></canvas>
            </div>
        </div>
        <p class="caption" style="text-align:center; max-width:640px; margin:0.8rem auto 0;">
            Given a set of images, VGGT predicts per-frame depth maps and
            camera poses.
        </p>
    </div>
</section>

<!-- ============================================================ -->
<!-- SECTION 4b: EVALUATION PROTOCOL -->
<!-- ============================================================ -->
<section id="evaluation" class="section">
    <div class="container">
        <h2>We develop a 'zero-shot' evaluation framework</h2>
        <p class="section-subtitle">
            We present the same oddity trials to both VGGT and human observers.
            For the model, we leverage its internal uncertainty&mdash;a confidence
            measure built into the training objective&mdash;to identify the odd
            object out. No fine-tuning, no task-specific training.
        </p>
        <div class="figure-single">
            <img src="static/media/fig2_protocol.png" alt="Evaluation protocol: encode image pairs, extract pairwise uncertainty, identify the non-matching object as the pair with lowest confidence" class="figure-img" style="max-width:100%;">
            <p class="caption">
                <strong>Top:</strong> For each trial, we encode all image pairs
                and extract the model&rsquo;s uncertainty. The non-matching object (B)
                is identified as the lowest-confidence pair; the confidence
                margin (&Delta;) provides a continuous measure of decision strength.
            </p>
        </div>
    </div>
</section>

<!-- ============================================================ -->
<!-- SECTION 5: RESULTS (the payoff) -->
<!-- ============================================================ -->
<section id="results" class="section alt-bg">
    <div class="container">
        <h2>Multi-view models predict human perceptual dynamics zero-shot</h2>
        <p>
            Across three independent measures, the model&rsquo;s behavior
            corresponds to human perception&mdash;not just <em>what</em>
            decisions are made, but <em>how difficult</em> each trial is
            and <em>how long</em> the underlying computation takes.
        </p>

        <div class="results-triptych">
            <div class="result-panel">
                <h3>Accuracy</h3>
                <div class="chart-container">
                    <canvas id="chart-accuracy" width="320" height="260"></canvas>
                </div>
                <p class="caption">VGGT matches human accuracy (79% vs. 78%) and dramatically outperforms DINOv2 (26%).</p>
            </div>
            <div class="result-panel">
                <h3>Error patterns</h3>
                <div class="chart-container">
                    <canvas id="chart-confidence" width="320" height="260"></canvas>
                </div>
                <p class="caption">Model confidence predicts human accuracy trial-by-trial (<em>r</em>&nbsp;=&nbsp;0.83, <em>R</em>&sup2;&nbsp;=&nbsp;0.69).</p>
            </div>
            <div class="result-panel">
                <h3>Reaction time</h3>
                <div class="chart-container">
                    <canvas id="chart-rt" width="320" height="260"></canvas>
                </div>
                <p class="caption">Model solution layer predicts human RT (<em>&rho;</em>&nbsp;=&nbsp;0.80)&mdash;deeper processing corresponds to slower responses.</p>
            </div>
        </div>

        <div class="results-summary">
            <p>
                These correspondences were never optimized for&mdash;they emerge
                from multi-view learning alone. The model uses only its pre-trained
                representations, with no task-specific training or fine-tuning.
            </p>
        </div>
    </div>
</section>

<!-- ============================================================ -->
<!-- SECTION 6: ATTENTION VISUALIZATIONS -->
<!-- ============================================================ -->
<section id="attention" class="section">
    <div class="container">
        <h2>How do these multi-view models determine object similarity?</h2>
        <p class="section-subtitle">
            These behavioral correspondences raise a mechanistic question: what kind of internal 
            representation supports this human-like performance?
            We visualize cross-image attention: for each query point on image A,
            we show where the model attends on the matching object (A&prime;)
            versus the non-matching object (B).
        </p>

        <!-- Controls -->
        <div class="attention-controls">
            <label>Trial:
                <select id="attn-trial-select"></select>
            </label>
            <div class="layer-slider-container">
                <label>Layer: <span id="layer-label">0</span></label>
                <input type="range" id="layer-slider" min="0" max="23" value="12" step="1">
                <div class="layer-labels">
                    <span>0 (early)</span>
                    <span>12</span>
                    <span>23 (late)</span>
                </div>
            </div>
            <label class="attn-toggle">
                <input type="checkbox" id="attn-mask-toggle"> Object mask
            </label>
        </div>

        <!-- Attention visualization grid -->
        <div class="attn-grid">
            <!-- Source image A -->
            <div class="attn-col">
                <div class="attn-col-header">Source (A)</div>
                <canvas id="attn-source" width="518" height="518"></canvas>
            </div>
            <!-- Reference images A' and B -->
            <div class="attn-col">
                <div class="attn-col-header">A&prime; (match)</div>
                <canvas id="attn-ref-match" width="180" height="180"></canvas>
                <div class="attn-col-header" style="margin-top:0.5rem">B (oddity)</div>
                <canvas id="attn-ref-nonmatch" width="180" height="180"></canvas>
            </div>
            <!-- Heatmap columns (one per query point, generated by JS) -->
            <div id="attn-heatmap-cols" class="attn-heatmap-cols">
                <!-- JS will insert columns here -->
            </div>
        </div>

        <p class="section-note">
            In early layers, attention is diffuse and undifferentiated. By intermediate
            layers, each query point on A elicits attention to the <em>corresponding
            spatial location</em> on A&prime;&mdash;the same part of the object, seen
            from a different angle. The model represents object similarity not through
            abstract features, but through concrete spatial correspondence.
        </p>
    </div>
</section>

<!-- ============================================================ -->
<!-- SECTION 7: CONCLUSION -->
<!-- ============================================================ -->
<section id="conclusion" class="section alt-bg">
    <div class="container">
        <h2>Bridging cognitive theory and machine learning</h2>
        <p>
            We find that neural networks trained on multi-view correspondence 
            &mdash;with no exposure to human experimental data&mdash;
            predicts the accuracy, error patterns, and temporal dynamics of human 3D shape inferences. 
            Critically, our zero-shot evaluation approach rules out the possibility
            that this correspondence is an artifact of task-specific
            training or linear re-weighting. 
            Rather, the design and optimization of this model leads to a natural alignment with human behavior.
        </p>
        <p>
            These findings provide a computational bridge between cognitive
            theory and current practice in deep learning. The empiricist claim
            that perception emerges from general-purpose learning over structured
            sensory experience&mdash;rather than from innate, domain-specific
            knowledge&mdash;is, in many ways, a precursor to the pprevailing
            deep learning paradigm. The emergent alignment between multi-view transformers
            and human perception suggests that this is not only an intellectual
            lineage, but an opportunity to formalize and evaluate longstanding theories of human perception.   
        </p>
    </div>
</section>

<!-- ============================================================ -->
<!-- SECTION 8: OPEN QUESTIONS -->
<!-- ============================================================ -->
<section id="open-questions" class="section">
    <div class="container narrow">
        <h2>Open questions + collaborative opportunities </h2>
        <p>
            These findings indicate that human-level 3D perception can emerge
            from simple, scalable learning over structured visual-spatial
            data&mdash;consistent with empiricist theories of perceptual
            development. This raises exciting questions  
            at the intersection of neurosciece, cognitive science, and computer science:
        </p>

        <div class="open-questions-list">
            <div class="oq-item">
                <h3>Biological constraints on model architecture</h3>
                <p>
                    While VGGT&rsquo;s feedforward architecture reflects some
                    aspects of neural processing, it lacks foveal sampling,
                    recurrent dynamics, and other biological constraints. 
                    Can we design models to reflect these architectural constraints? 
                </p>
            </div>

            <div class="oq-item">
                <h3>Ecologically plausible training data</h3>
                <p>
                    Current multi-view models train on internet-scale datasets that are analogous to,
                    but quite different from, the visual experience available to developing infants.
                    What learn algorithms are needed to learn from infant-scale data?
                </p>
            </div>

            <div class="oq-item">
                <h3>Underlying design principles</h3>
                <p>
                    What learning objectives and data distributions are essential for human-like
                    3D perception, and which are redundant? Systematic ablations
                    could identify the minimal recipe that produces human-level
                    behavior.
                </p>
            </div>
        </div>
    </div>
</section>

<!-- ============================================================ -->
<!-- FOOTER -->
<!-- ============================================================ -->
<footer>
    <div class="container narrow">
        <h2>Citation</h2>
        <pre class="citation"><code>@article{bonnen2025multiview,
  title={Human-level 3D shape perception emerges from multi-view learning},
  author={Bonnen, Tyler and Malik, Jitendra and Kanazawa, Angjoo},
  year={2025}
}</code></pre>
        <div class="footer-links">
            <a href="https://github.com/tzler/human_multiview">Code &amp; Analysis</a>
            <a href="https://huggingface.co/datasets/tzler/MOCHI">MOCHI Dataset</a>
        </div>
        <p class="acknowledgments">
            This work is supported by the NIH (Award F99NS125816) and the UC Presidential
            Postdoctoral Fellowship Award.
        </p>
    </div>
</footer>

<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/controls/OrbitControls.js"></script>
<script src="static/js/camera_viz.js?v=5"></script>
<script src="static/js/main.js?v=37"></script>
</body>
</html>
